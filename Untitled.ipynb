{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc0167f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "myRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6dba9888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "myRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c11c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "myRDD.reduce(lambda x, y : x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c9a03bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.parallelize([(1,2), (3,4), (3,6)])\n",
    "myRDD.reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ade4d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "myRDD.saveAsTextFile('file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de4fe1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3, 4], [5], [6, 7], [8], [9, 10], [11], [12, 13]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13],9)\n",
    "# myRDD.collect()\n",
    "myRDD.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea4a167c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Java', 20000), ('Python', 100000), ('Scala', 3000)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd = sc.parallelize(dataList)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "280774a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ['a', 'b']), (2, ['c'])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.parallelize([(1,'a'), (2,'c'), (1,'b')])\n",
    "# myRDD.groupByKey().map(lambda x : (x[0], list(x[1]))).collect()\n",
    "myRDD.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4be2291c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 13, 14]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rdd = sc.parallelize([1,2,3,4])\n",
    "my_rdd.map(lambda x: x+ 10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93f0a7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_rdd = sc.parallelize([2, 3, 4, 5, 6, 7])\n",
    "filter_rdd.filter(lambda x: x%2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0aa468d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rahul', 'Rohan']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_rdd = sc.parallelize(['Rahul', 'Swati', 'Rohan', 'Shreya', 'Priya'])\n",
    "filter_rdd.filter(lambda x: x.startswith('R')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a30238d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 12, 32, 6, 9, 12]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([2,4,5,6,7,8,9,12,23,13,11,19,32])\n",
    "union_1 = data.filter(lambda x: x % 2 == 0)\n",
    "union_2 = data.filter(lambda x: x % 3 == 0)\n",
    "union_1.union(union_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "953f6f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey', 'there', 'welcome', 'This', 'is', 'PySpark', 'RDD', 'Transformations']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([\"Hey there\",\"welcome\",\"This is PySpark RDD\",\"Transformations\"])\n",
    "data.flatMap(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "569b24a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Shreya', 50),\n",
       " ('Swati', 19),\n",
       " ('Rahul', 48),\n",
       " ('Swat', 26),\n",
       " ('Abhay', 55),\n",
       " ('Rohan', 44)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([('Rahul', 25), ('Swat', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "data.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05a1937b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abhay', 29),\n",
       " ('Abhay', 26),\n",
       " ('Rahul', 25),\n",
       " ('Rahul', 23),\n",
       " ('Rohan', 22),\n",
       " ('Rohan', 22),\n",
       " ('Shreya', 22),\n",
       " ('Shreya', 28),\n",
       " ('Swati', 26),\n",
       " ('Swati', 19)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "rdd.sortByKey('ascending').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd067dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shreya [22, 28]\n",
      "Swati [26, 19]\n",
      "Rahul [25, 23]\n",
      "Abhay [29, 26]\n",
      "Rohan [22, 22]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "data = rdd.groupByKey().collect()\n",
    "for key, value in data:\n",
    "    print(key, list(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88687efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rahul 2\n",
      "Swati 2\n",
      "Rohan 2\n",
      "Shreya 1\n",
      "Abhay 1\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "data = rdd.countByKey().items()\n",
    "for key, value in data:\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b348a1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+-------+---------+\n",
      "|Division|English|Mathematics|Physics|Chemistry|\n",
      "+--------+-------+-----------+-------+---------+\n",
      "|       C|     85|         76|     87|       91|\n",
      "|       B|     85|         76|     87|       91|\n",
      "|       A|     85|         78|     96|       92|\n",
      "|       A|     92|         76|     89|       96|\n",
      "+--------+-------+-----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('PySpark DataFrame From RDD').getOrCreate()\n",
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), ('A', 92,76,89,96)], 4)\n",
    "#print(type(rdd))\n",
    "sub = ['Division','English','Mathematics','Physics','Chemistry']\n",
    "df = spark.createDataFrame(rdd, schema=sub)\n",
    "#print(type(df))\n",
    "#df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dfa5d91f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/01 00:04:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , X, point, int, short, tall, pe, pcgrass, up, bg, water\n",
      " Schema: _c0, X, point, int, short, tall, pe, pcgrass, up, bg, water\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/spark-3.3.0-bin-hadoop3/samplefiles/allveg.csv\n",
      "+---+---+--------+----+-----+----+---+-------+---+---+-----+\n",
      "|_c0|  X|   point| int|short|tall| pe|pcgrass| up| bg|water|\n",
      "+---+---+--------+----+-----+----+---+-------+---+---+-----+\n",
      "|  1|  1|   AV067|0.05|   85|   0| NA|     NA|  0|  0|    5|\n",
      "|  2|  2| LBKIRA3|0.04|   97|   0| NA|     NA|  0|  0|    3|\n",
      "|  3|  3| LBKIRA4|0.14|   99|   0| NA|     NA|  0|  0|    1|\n",
      "|  4|  4|NVVEG023|   0|    0| 100| NA|     NA|  0|  0|    0|\n",
      "|  5|  5|NVVEG031|0.14|    5|  90| NA|     NA|  0|  0|    5|\n",
      "+---+---+--------+----+-----+----+---+-------+---+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('samplefiles/allveg.csv', header = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d6e4052d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|liters|\n",
      "+------+\n",
      "|     5|\n",
      "|     3|\n",
      "|     1|\n",
      "|     0|\n",
      "|     5|\n",
      "|     0|\n",
      "|     1|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     5|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('samplefiles/allveg.csv', header = True)\n",
    "df = df.drop('_c0')\n",
    "df = df.withColumn('Water in ml', df.water*1000)\n",
    "df = df.withColumnRenamed(\"water\", \"water in l\")\n",
    "df.select(df.point, df['water in ml'])\n",
    "df.select(df['water in l'].alias(\"liters\")).show()\n",
    "# df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fd5d1c63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- batters: struct (nullable = true)\n",
      " |    |-- batter: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ppu: double (nullable = true)\n",
      " |-- topping: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.read.text('samplefiles/sample1.txt')\n",
    "df = spark.read.json('samplefiles/Fig4c.json',multiLine=True)\n",
    "# type(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9530ba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/01 01:10:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , X, point, int, short, tall, up, bg, water\n",
      " Schema: _c0, X, point, int, short, tall, up, bg, water\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/spark-3.3.0-bin-hadoop3/samplefiles/allveg.csv\n",
      "+---+---+--------+----+-----+----+---+---+-----+\n",
      "|_c0|  X|   point| int|short|tall| up| bg|water|\n",
      "+---+---+--------+----+-----+----+---+---+-----+\n",
      "|  1|  1|   AV067|0.05|   85|   0|  0|  0|    5|\n",
      "|  2|  2| LBKIRA3|0.04|   97|   0|  0|  0|    3|\n",
      "|  3|  3| LBKIRA4|0.14|   99|   0|  0|  0|    1|\n",
      "|  4|  4|NVVEG023|   0|    0| 100|  0|  0|    0|\n",
      "|  5|  5|NVVEG031|0.14|    5|  90|  0|  0|    5|\n",
      "|  6|  6|NVVEG024|   0|    0| 100|  0|  0|    0|\n",
      "|  7|  7|NVVEG029|0.02|   10|  49|  0|  0|    1|\n",
      "|  8|  8|NVVEG030|   0|    5|  60|  0|  0|    0|\n",
      "|  9|  9|NVVEG032|   0|   30|  35|  0|  0|    0|\n",
      "| 10| 10|NVVEG022|   0|    0|  70| 15|  0|    0|\n",
      "| 11| 11|NVVEG033|   0|    5|  20|  0|  0|    0|\n",
      "| 12| 12|NVVEG028|0.13|   15|  25| 30|  0|    5|\n",
      "| 13| 13|NVVEG025|   0|   10|  30| 45|  0|    0|\n",
      "| 14| 14|NVVEG035|   0|   10|  55| 20|  0|    0|\n",
      "| 15| 15|NVVEG034|   0|   10|  60|  0|  0|    0|\n",
      "| 16| 16|NVVEG038|   0|    0|   0| 97|  0|    0|\n",
      "| 17| 17|NVVEG027|   0|    0|   0| 99|  0|    0|\n",
      "| 18| 18|NVVEG039|   0|    0|   0| 95|  0|    0|\n",
      "| 19| 19|NVVEG026|   0|    0|   0| 94|  0|    0|\n",
      "| 20| 20|NVVEG036|   0|    0|  30| 60|  0|    0|\n",
      "+---+---+--------+----+-----+----+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = ['samplefiles/allveg.csv', 'samplefiles/cars.csv']\n",
    "df = spark.read.csv(files, sep = ',' ,  inferSchema=True, header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8a936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
